{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests #gets the HTML \n",
    "import lxml.html as lh #parsing \n",
    "import pandas as pd #datasets\n",
    "from bs4 import BeautifulSoup # html parsing\n",
    "from requests_html import AsyncHTMLSession # async session\n",
    "from requests_html import HTMLSession # sync session\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the content of a web page trough the requests API's get() method. We desire to parse the HTML to retrieve the table containing our data. However, the data in inside a table which gets rendered by the browser, so we can't simply dig into the HTML. Let's start by installing the requests_html package. To install, simply write pip install requests_html in your shell. I'll use AsyncHTMLSession since I'm working with Jupyter, but you can use HTMLSession aswell. After retrieving the page, we render the javascript code to obtain the complete HTML code. I'll post both the asynchornous function (the one I'm using currently) and the standard function (if you might need it). \n",
    "\n",
    "\n",
    "\n",
    "To do this, we use the BeautifulSoulp4 library. After generating a bs4 object, you can either inspect the html trough your browser or using the prettify() method of bs4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve a beautifoulsoup object with the HTML of the url (asynchronous)\n",
    "async def asyncHTMLfromURL(url):\n",
    "    # create an HTML asynchornous session\n",
    "    asession = AsyncHTMLSession()\n",
    "    #use the session object to connect to the page\n",
    "    page = await asession.get(url)\n",
    "    # return the HTTP response code\n",
    "    print('HTTP response: ' + str(page.status_code))    # 200: OK; 204: NO CONTENT\n",
    "    # Run JavaScript code on webpage (sleep is for the loading time of the contents)\n",
    "    await page.html.arender(sleep=5)\n",
    "\n",
    "    # create a bs4 object over the html of the page\n",
    "    soup = BeautifulSoup(page.html.html, 'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve a beautifoulsoup object with the HTML of the url)\n",
    "def HTMLfromURL(url):\n",
    "    # create an HTML asynchornous session\n",
    "    session = HTMLSession()\n",
    "    #use the session object to connect to the page\n",
    "    page = session.get(url)\n",
    "    # return the HTTP response code\n",
    "    print('HTTP response: ' + str(page.status_code))   # 200: OK; 204: NO CONTENT\n",
    "    # Run JavaScript code on webpage (sleep is for the loading time of the contents)\n",
    "    page.html.render(sleep=4.5)\n",
    "\n",
    "    # create a bs4 object over the html of the page\n",
    "    soup = BeautifulSoup(page.html.html, 'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We locate our target data over the table with class 'days ng-star-inserted'.  after retrieving the table, we can easily access the headings and the body of our table. We can notice that there's an inner table, so we locate the html and estract the data from the various rows. We also procees to split the multi-rows which contains 'Max', 'Avg' and 'Min' of various features, and we reallocate them as new features. I preferred to do this manually because extracting from the <td> ws a bit tricky for this table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataRetrieval(soup):\n",
    "    #retrieve a bs4 object containing our table \n",
    "    weather_table = soup.find('table', class_='days ng-star-inserted')\n",
    "    #retrieve the headings of the table\n",
    "    weather_table_head = weather_table.thead.find_all(\"tr\")\n",
    "\n",
    "    # Get all the headings\n",
    "    headings = []\n",
    "    for td in weather_table_head[0].find_all(\"td\"):\n",
    "        headings.append(td.text)\n",
    "    #print(headings)\n",
    "\n",
    "\n",
    "    ## retrieve the data from the table\n",
    "    column, data = [], []\n",
    "    weather_table_data = weather_table.tbody.find_all('table') # find inner tables\n",
    "    for table in weather_table_data:\n",
    "        weather_table_rows = table.find_all('tr') # find rows\n",
    "        for row in weather_table_rows:  \n",
    "            column.append(row.text.strip()) #take data without spaces \n",
    "        data.append(column)\n",
    "        column = []\n",
    "    # slice the triple rows into sub-wors\n",
    "    datas = slicing(data, column, headings)\n",
    "    return datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split triples into single features\n",
    "def slicing(data, column, headings):\n",
    "     \n",
    "    datas, col, colmax, colavg, colminy = [], [],[],[], []\n",
    "    split, first, i = False, True, 0\n",
    "    \n",
    "    for column in data:\n",
    "        for entry in column:\n",
    "            for c in entry:\n",
    "                if c == ' ':\n",
    "                    split = True\n",
    "            if split:\n",
    "                maxy, avg, miny = entry.split()\n",
    "                if first:\n",
    "                    colmax.append(headings[i]+ ' '+ maxy)\n",
    "                    colavg.append(headings[i]+ ' '+ avg)\n",
    "                    colminy.append(headings[i]+ ' '+ miny)\n",
    "                    first = False\n",
    "                else:\n",
    "                    colmax.append(maxy)\n",
    "                    colavg.append(avg)\n",
    "                    colminy.append(miny)\n",
    "            elif first:\n",
    "                col.append(headings[i]+ ' '+entry)\n",
    "                first = False\n",
    "            else: col.append(entry)\n",
    "        if split:\n",
    "            datas.append(colmax)\n",
    "            datas.append(colavg)\n",
    "            datas.append(colminy)\n",
    "            colmax, colavg, colminy = [],[],[]\n",
    "            split = False\n",
    "        else: \n",
    "            datas.append(col)\n",
    "            col = []\n",
    "        first = True\n",
    "        i +=1 \n",
    "    return datas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reformat the data to be more similar to the water level dataset and also more compact, so it can store both day, month and year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reformat the data section to be similar to the water level datas (days/month/year)\n",
    "def dataReformat(data, year, month):\n",
    "    data[0][0] = 'Dates'\n",
    "    for i in range(1, len(data[0])):\n",
    "        if len(data[0][i]) == 2:\n",
    "            if (len(str(month)) == 2):\n",
    "                data[0][i] = data[0][i] + '/' + str(month) + '/' + str(year)\n",
    "            else: data[0][i] = data[0][i] + '/' + '0' + str(month) + '/' + str(year)\n",
    "        elif (len(str(month)) == 2):\n",
    "                data[0][i] = '0' + data[0][i] + '/' + str(month) + '/' + str(year)\n",
    "        else: data[0][i] = '0' + data[0][i] + '/' + '0' + str(month) + '/' + str(year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now retrieve as much data as possible. The more the better. We start from 1997 (the first weather data available on the site for venice) and go up to 2018 (this is due to the water level data which goes up to 2018). We procees to store all of our data in a pandas DataFrame. This might take a while so be aware of it.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## double for - retrieve the various year's data\n",
    "first = True\n",
    "for year in range (1997, 2019):\n",
    "    for month in range (1, 13):\n",
    "        print('Retrieving weather data of '+str(month)+'/'+str(year))\n",
    "        # URL of the page we want to retrieve\n",
    "        url='https://www.wunderground.com/history/monthly/it/venice/date/'+str(year)+'-'+str(month)\n",
    "        # Retrieve the HTML form the url\n",
    "        soup = await asyncHTMLfromURL(url)\n",
    "        # retrieve the data from the HTML\n",
    "        fulldata = dataRetrieval(soup)\n",
    "        # reformat the data section to be similar to the water level datas\n",
    "        dataReformat(fulldata, year, month)\n",
    "        dataframe = np.array([item[1:] for item in fulldata]).transpose()\n",
    "        if first:\n",
    "            df = pd.DataFrame(dataframe, columns=([item[0] for item in fulldata]))\n",
    "            first = False\n",
    "        else:\n",
    "            df2 = pd.DataFrame(dataframe, columns=([item[0] for item in fulldata]))\n",
    "            df = df.append(df2, ignore_index=True)\n",
    "        print('weather data of '+str(month)+'/'+str(year)+' retrieved successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to save our dataframe so that we don't actually have to download it every single time. I'm going to use a .CSV format which is pretty standard way to save a dataset. This can be done easily trough the 'to_csv()' function from pandas.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save dataframe as .csv\n",
    "df.to_csv(r'.\\weather_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've prepared our hystorical weather data of Venice, we want to obtain and prepare a clean dataset of the sea level over the last few years. Luckily, we can find the data we need in the following page https://www.comune.venezia.it/it/content/archivio-storico-livello-marea-venezia-1 .\n",
    "We need however to download every .csv and rearrange them in a single dataset, which we'll later join with the other dataset we've generated previously. We could do this manually, but let's use some python magic to automate the procedure! Once again to move around the various urls we can jsut change the year from the url, and trough the request.get() function we can download every .csv which we'll then write on the disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statistics import mean, median\n",
    "\n",
    "## from every url download and save on disk the .csv \n",
    "for year in range (1997, 2019):\n",
    "    url = 'https://www.comune.venezia.it/sites/comune.venezia.it/files/documenti/centro_maree/archivioDati/valoriorari_puntasalute_'+str(year)+'.csv'\n",
    "    obj = requests.get(url)\n",
    "\n",
    "    with open('./sea_level/'+str(year)+'_sea_levels.csv', 'wb') as file:\n",
    "        file.write(obj.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much quicker than do it manually right? \n",
    "By checking our .csv, we discover that the data are arranged hourly for each day, while we only have one single weather value for every day. To play around this, we can extract the maximum, the minium and the average of the sea levels for each day. (Note that the 2015 files has some rows that contains plain text at the end that needs to be removed!)\n",
    "To load the csv we can use Pandas. Since the file use the ';' as separator, we need to specify it (since pandas usually have ',' as default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sea = pd.read_csv('./sea_level/'+str(2018)+'_sea_levels.csv', sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea now is to cycle between every year, extract the info we need for every set of days and re-arrange everything in a new dataset. The data column name change from 2016 onwards from 'Data' to 'data' and 'GIORNO', so keep that in mind while extracting the data. Using functions will make the code cleaner, so don't just paste the same code over and over! Moreover, the year 2018 has metres as a mestric, so we'll adapt it to cm as the rest of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## create a new dataset using the unorganized data \n",
    "def sea_refactor(data_sea, day, first, df):\n",
    "    sea_level = []\n",
    "    for index, row in data_sea.iterrows():\n",
    "        # appends all the multiple data for single day\n",
    "        if (str(row['Date']) == day):\n",
    "            sea_level.append(row['Sea_level'])\n",
    "        else:\n",
    "            # only first occurance: create a new dataset containing rearranged data \n",
    "            if first:\n",
    "                df = pd.DataFrame(np.array([[day, min(sea_level), \"{0:.2f}\".format(mean(sea_level)), max(sea_level)]]), columns=['day', 'min_sea_level (cm)', 'avg_sea_level (cm)', 'max_sea_level (cm)'])\n",
    "                first = False\n",
    "            # append to the previous dataset the new row with rearranged data   \n",
    "            else:\n",
    "                df2 = pd.DataFrame(np.array([[day, min(sea_level), \"{0:.2f}\".format(mean(sea_level)), max(sea_level)]]), columns=(['day', 'min_sea_level (cm)', 'avg_sea_level (cm)', 'max_sea_level (cm)']))\n",
    "                df = df.append(df2, ignore_index=True)\n",
    "            # update indexes\n",
    "            day = str(row['Date'])\n",
    "            sea_level = []\n",
    "            sea_level.append(row['Sea_level']) \n",
    "    return df\n",
    "\n",
    "## change the metric from m to cm \n",
    "def sea_leveller(dataset):                             \n",
    "    for index, row in dataset.iterrows():\n",
    "        dataset.at[index, 'Sea_level'] = (row['Sea_level']*100)\n",
    "    return dataset\n",
    "\n",
    "## change the format of the year over the date for the whole dataset\n",
    "def year_refactor(dataset):\n",
    "    for index, row in dataset.iterrows():\n",
    "        day, month, _ = row['Date'].split('/')\n",
    "        dataset.at[index, 'Date'] = (str(day)+'/'+str(month)+'/'+str(2007))\n",
    "\n",
    "# change the format of the date for the whole dataset\n",
    "def date_reformer(dataset, year):\n",
    "    for index, row in dataset.iterrows():\n",
    "        day, month, _ = row['Date'].split('-')\n",
    "        if len(str(day)) == 1: day = '0'+str(day)\n",
    "        if str(month) == 'gen': dataset.at[index, 'Date'] = (str(day)+'/0'+str(1)+'/'+str(year))\n",
    "        elif str(month) == 'feb': dataset.at[index, 'Date'] = (str(day)+'/0'+str(2)+'/'+str(year))\n",
    "        elif str(month) == 'mar': dataset.at[index, 'Date'] = (str(day)+'/0'+str(3)+'/'+str(year))\n",
    "        elif str(month) == 'apr': dataset.at[index, 'Date'] = (str(day)+'/0'+str(4)+'/'+str(year))\n",
    "        elif str(month) == 'mag': dataset.at[index, 'Date'] = (str(day)+'/0'+str(5)+'/'+str(year))\n",
    "        elif str(month) == 'giu': dataset.at[index, 'Date'] = (str(day)+'/0'+str(6)+'/'+str(year))\n",
    "        elif str(month) == 'lug': dataset.at[index, 'Date'] = (str(day)+'/0'+str(7)+'/'+str(year))\n",
    "        elif str(month) == 'ago': dataset.at[index, 'Date'] = (str(day)+'/0'+str(8)+'/'+str(year))\n",
    "        elif str(month) == 'set': dataset.at[index, 'Date'] = (str(day)+'/0'+str(9)+'/'+str(year))\n",
    "        elif str(month) == 'ott': dataset.at[index, 'Date'] = (str(day)+'/'+str(10)+'/'+str(year))\n",
    "        elif str(month) == 'nov': dataset.at[index, 'Date'] = (str(day)+'/'+str(11)+'/'+str(year))\n",
    "        elif str(month) == 'dic': dataset.at[index, 'Date'] = (str(day)+'/'+str(12)+'/'+str(year))\n",
    "    return dataset\n",
    "\n",
    "## generate a new dataset from previous disorganized data using previosuly defined functions\n",
    "first, df = True, []\n",
    "for year in range(1997, 2019):\n",
    "    # load the dataset\n",
    "    data_sea = pd.read_csv('./sea_level/'+str(year)+'_sea_levels.csv', sep=\";\")\n",
    "    day = '01/01/'+str(year)\n",
    "    # rename the columns of the dataset \n",
    "    data_sea = data_sea.rename(columns={str(data_sea.columns[0]):'Date', str(data_sea.columns[2]):'Sea_level'})\n",
    "\n",
    "    if year == 2018: data_sea = sea_leveller(data_sea)\n",
    "        \n",
    "    if year == 2007: year_refactor(data_sea)\n",
    "        \n",
    "    if year in [x for x in range(1997, 2010) if x != 2007]: \n",
    "        data_sea = date_reformer(data_sea, year)\n",
    "        \n",
    "    print('Processing the year '+str(year))\n",
    "    df = sea_refactor(data_sea, day, first, df)\n",
    "    print('Done processing the year '+str(year))\n",
    "    first = False      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we can save of dataframe as a .csv which we'll load later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save dataframe as .csv\n",
    "df.to_csv(r'.\\sea_level.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
